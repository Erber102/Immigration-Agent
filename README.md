# AI Immigration Agent

A modular scaffold for an immigration consulting agent that supports:
- FastAPI HTTP API for initial plan matching and follow‑up Q&A
- RAG-based Q&A with a pluggable LLM client
- Background data refresh pipeline (simple ReAct-style exploration, value assessment, and a JSON page registry)

The design separates the online API surface from the offline knowledge refresh pipeline for clarity and maintainability.

## Requirements
- Python 3.10+
- macOS/Linux/Windows

## Setup

1) Create a virtual environment and install dependencies:
```bash
python -m venv .venv && source .venv/bin/activate
pip install -r ai_immigration_agent/requirements.txt
```

2) Configure environment variables:
```bash
cd ai_immigration_agent
cp .env.template .env
# Edit .env to add your keys/paths
```

Key variables (see `.env.template`):
- `OPENAI_API_KEY`: Optional. Without it, the LLM client returns a deterministic mock response for local dev.
- `LLM_PROVIDER`, `LLM_MODEL`: Defaults to OpenAI and `gpt-4o-mini`.
- `CHROMA_DB_PATH`: Path to Chroma persistence folder. Falls back to in‑memory store if Chroma is unavailable.
- `PAGE_REGISTRY_PATH`: JSON file storing discovered pages and scores.
- `HOST`, `PORT`: API server bind host/port.

## Run the API

Start a minimal FastAPI app with a health endpoint:
```bash
python ai_immigration_agent/main.py
```
- Health check: `GET http://0.0.0.0:8000/health`

Optional endpoints for plan match and Q&A are implemented in `src/api/endpoints.py`. To enable them, mount the router in `main.py` like:
```python
from src.api.endpoints import router
app.include_router(router)
```

## Plan Matching and Q&A (Optional)
- `POST /api/v1/plans/match` → accepts a user profile JSON, returns `session_id` and plan matches (via `core.plan_matcher`).
- `POST /api/v1/qna` → accepts `session_id` and a question, returns an answer generated by RAG (`core.rag_core`).

`agent/qna_agent.py` keeps a minimal in‑memory session store for demo purposes.

## Pipeline: Knowledge Refresh

Run once manually:
```bash
python ai_immigration_agent/run_pipeline.py
```
What it does:
- `task_planner.get_tasks()` defines simple exploration goals.
- `agent_core.run_task()` performs a trivial search/fetch, calls `value_assessor.assess()` to score pages, and upserts them into `data_structures/page_registry.py` (JSON).
- Later you can chunk/vectorize these pages and push to the vector DB (`core/vector_db.py`).

Background scheduling (optional):
- See `src/pipeline/scheduler.py` for an APScheduler example to run the refresh job periodically.

## Development Notes

- LLM client (`core/llm_clients.py`) supports OpenAI; without an API key it returns a predictable mock so you can develop offline.
- Vector DB (`core/vector_db.py`) uses Chroma when available, otherwise falls back to a simple in‑memory list for development.
- RAG (`core/rag_core.py`) retrieves from the KB collection and composes a system prompt with context to generate answers.

## Next Steps
- Replace the stub `web_search` with a real search API.
- Add page chunking and embedding to populate Chroma collections (`kb`, `plans`).
- Persist sessions and user profiles in a database instead of memory.
- Expand the plan matcher prompts/embeddings for higher recall and precision.

## License
MIT
